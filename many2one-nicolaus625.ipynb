{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"many2one-nicolaus625.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc-autonumbering":false,"toc-showmarkdowntxt":false},"cells":[{"cell_type":"markdown","metadata":{"id":"V1FWITFR4kdt"},"source":["# Baseline"]},{"cell_type":"markdown","metadata":{"id":"sMrhEjZQNpaf"},"source":["## Load datasets"]},{"cell_type":"code","metadata":{"id":"culzXnwBrdi_"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive/') \n","# %%capture\n","# !tar -xzvf ../content/gdrive/MyDrive/IDL\\ 11785/project/knnw-720p.tar.gz -C ../content/gdrive/MyDrive/IDL\\ 11785/project/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrGwK-OQwcZT"},"source":["!/opt/bin/nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4x0fv3h9wgVU"},"source":["!kill -9 -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":525},"id":"9A1-MhdkIS85","executionInfo":{"status":"ok","timestamp":1619859464378,"user_tz":-480,"elapsed":164202,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}},"outputId":"1c3c0f8f-f895-4d9d-bd3c-75af9dfd8b5a"},"source":["!pip install torch===1.7.1 torchvision===0.8.2 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch===1.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n","\u001b[?25hCollecting torchvision===0.8.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/df/969e69a94cff1c8911acb0688117f95e1915becc1e01c73e7960a2c76ec8/torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8MB)\n","\u001b[K     |████████████████████████████████| 12.8MB 204kB/s \n","\u001b[?25hCollecting torchaudio===0.7.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/16/ecdb9eb09ec6b8133d6c9536ea9e49cd13c9b5873c8488b8b765a39028da/torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 54.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch===1.7.1) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch===1.7.1) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision===0.8.2) (7.1.2)\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchvision, torchaudio\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","Successfully installed torch-1.7.1 torchaudio-0.7.2 torchvision-0.8.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchvision"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVfVlcXvck7x","executionInfo":{"status":"ok","timestamp":1619860781169,"user_tz":-480,"elapsed":803,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}},"outputId":"1cfb1475-b248-4438-a82e-2ff5e14be7b5"},"source":["import torch\n","from torch import nn\n","from torch.utils import data\n","import torchvision as vis\n","import sys\n","\n","# torch.manual_seed(117850791)\n","is_windows = sys.platform == \"win32\"\n","has_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if has_cuda else \"cpu\")\n","device"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"i6Bo8EOtoar2","executionInfo":{"status":"ok","timestamp":1619860654860,"user_tz":-480,"elapsed":796,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["from PIL import Image\n","import os\n","\n","class FlatImageData(vis.datasets.VisionDataset):\n","  def __init__(self, root, transform, validation_reserved_images=25947, win_len=3):\n","    self.root = root\n","    self.images = os.listdir(root)\n","    self.images.sort(key=lambda x: int(x[6:-5]))# sort by frame no.\n","    self.transform = transform\n","    self.training_mode = True\n","    self.reserved_images = validation_reserved_images\n","    self.win_len = 3\n","        \n","  def __len__(self):\n","    if self.training_mode:\n","      return len(self.images) - self.reserved_images - self.win_len + 1\n","    else:\n","      return self.reserved_images - self.win_len + 1\n","    \n","  def pil_loader(self, path: str) -> Image.Image:\n","    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n","    with open(path, 'rb') as f:\n","        img = Image.open(f)\n","        return img.convert('RGB')\n","\n","  def __getitem__(self, index):\n","    if self.training_mode:\n","      index += self.reserved_images\n","    \n","    image_name = [self.images[index+i] for i in range(self.win_len)]\n","    image_path = [f\"{self.root}/{i}\" for i in image_name]\n","    image = [self.pil_loader(img) for img in image_path]\n","    if self.transform is not None:\n","         image = [self.transform(img) for img in image]\n","    return image\n","\n","  def collate_fn(batch):\n","      return torch.as_tensor(batch)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6Iqt5YAoar3","executionInfo":{"status":"ok","timestamp":1619860686396,"user_tz":-480,"elapsed":1003,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}},"outputId":"7aea0d6f-a656-41ed-c146-d82014c48f2a"},"source":["PATH = \"/content/gdrive/MyDrive/IDL 11785/project/knnw-720p\"\n","dataset = FlatImageData(root=PATH, #\"/home/ubuntu/data/knnw-720p\",\n","                             transform=vis.transforms.Compose([\n","                               vis.transforms.RandomHorizontalFlip(),\n","                               vis.transforms.RandomApply(nn.ModuleList([\n","                                 vis.transforms.RandomAffine(degrees=15),\n","                                 vis.transforms.CenterCrop((1024, 576))\n","                               ]), p=0.5),\n","                               vis.transforms.ToTensor(),\n","                               nn.AdaptiveAvgPool2d((128, 128))\n","                             ])\n","                            )\n","dataset"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset FlatImageData\n","    Number of datapoints: 12408\n","    Root location: /content/gdrive/MyDrive/IDL 11785/project/knnw-720p"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"cjCIyjpWdqGw"},"source":["## Train Model"]},{"cell_type":"code","metadata":{"id":"52LRNqjIoar4","executionInfo":{"status":"ok","timestamp":1619859489649,"user_tz":-480,"elapsed":1124,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["import os\n","model_store = \"model_checkpoints\"\n","\n","class StoredModel:\n","  def __init__(self, model, optimizer, scheduler, criterion):\n","    self.model = model\n","    self.optimizer = optimizer\n","    self.scheduler = scheduler\n","    self.criterion = criterion"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"wo1FBitaoar4","executionInfo":{"status":"ok","timestamp":1619862435121,"user_tz":-480,"elapsed":825,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["from torch.nn import functional as F\n","from typing import List, Callable, Union, Any, TypeVar, Tuple\n","Tensor = TypeVar('torch.tensor')\n","\n","class BetaVAE(nn.Module):\n","\n","    num_iter = 0 # Global static variable to keep track of iterations\n","\n","    def __init__(self,\n","                 in_channels: int,\n","                 latent_dim: int,\n","                 hidden_dims: List = None,\n","                 beta: int = 4,\n","                 gamma:float = 1000.,\n","                 max_capacity: int = 25,\n","                 Capacity_max_iter: int = 1e5,\n","                 loss_type:str = 'B',\n","                 **kwargs) -> None:\n","        super(BetaVAE, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        self.beta = beta\n","        self.gamma = gamma\n","        self.loss_type = loss_type\n","        self.C_max = torch.Tensor([max_capacity])\n","        self.C_stop_iter = Capacity_max_iter\n","\n","        modules = []\n","        if hidden_dims is None:\n","            hidden_dims = [32, 64, 128, 256, 512]\n","\n","        # Build Encoder\n","        for h_dim in hidden_dims:\n","            modules.append(\n","                nn.Sequential(\n","                    nn.Conv2d(in_channels, out_channels=h_dim,\n","                              kernel_size= 3, stride= 2, padding  = 1),\n","                    nn.BatchNorm2d(h_dim),\n","                    nn.LeakyReLU())\n","            )\n","            in_channels = h_dim\n","            \n","        modules.append(nn.Flatten())\n","\n","        self.encoder = nn.Sequential(*modules)\n","        \n","        self.fc_mu = nn.Linear(hidden_dims[-1]*16*3, latent_dim)\n","        self.fc_var = nn.Linear(hidden_dims[-1]*16*3, latent_dim)\n","\n","\n","        # Build Decoder\n","        modules = []\n","\n","        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 16)\n","\n","        hidden_dims.reverse()\n","\n","        for i in range(len(hidden_dims) - 1):\n","            modules.append(\n","                nn.Sequential(\n","                    nn.ConvTranspose2d(hidden_dims[i],\n","                                       hidden_dims[i + 1],\n","                                       kernel_size=3,\n","                                       stride = 2,\n","                                       padding=1,\n","                                       output_padding=1),\n","                    nn.BatchNorm2d(hidden_dims[i + 1]),\n","                    nn.LeakyReLU())\n","            )\n","\n","\n","\n","        self.decoder = nn.Sequential(*modules)\n","\n","        self.final_layer = nn.Sequential(\n","                            nn.ConvTranspose2d(hidden_dims[-1],\n","                                               hidden_dims[-1],\n","                                               kernel_size=3,\n","                                               stride=2,\n","                                               padding=1,\n","                                               output_padding=1),\n","                            nn.BatchNorm2d(hidden_dims[-1]),\n","                            nn.LeakyReLU(),\n","                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n","                                      kernel_size= 3, padding= 1),\n","                            nn.Tanh())\n","\n","    def encode(self, inputs: Tensor) -> List[Tensor]:\n","        \"\"\"\n","        Encodes the input by passing through the encoder network\n","        and returns the latent codes.\n","        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n","        :return: (Tensor) List of latent codes\n","        \"\"\"\n","        result = torch.cat([self.encoder(input) for input in inputs], dim=1)\n","\n","        # Split the result into mu and var components\n","        # of the latent Gaussian distribution\n","        mu = self.fc_mu(result)\n","        log_var = self.fc_var(result)\n","\n","        return (inputs, mu, log_var)\n","\n","    def decode(self, z: Tensor) -> Tensor:\n","        result = self.decoder_input(z)\n","        result = result.view(-1, 512, 4, 4)\n","        result = self.decoder(result)\n","        result = self.final_layer(result)\n","        return result\n","\n","    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n","        \"\"\"\n","        Will a single z be enough ti compute the expectation\n","        for the loss??\n","        :param mu: (Tensor) Mean of the latent Gaussian\n","        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n","        :return:\n","        \"\"\"\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return eps * std + mu\n","\n","    def forward(self, inputs: Tensor, **kwargs) -> Tensor:\n","        pooled_inputs, mu, log_var = self.encode(inputs)\n","        z = self.reparameterize(mu, log_var)\n","        \n","        self.current_inputs = pooled_inputs\n","        self.current_mu = mu\n","        self.current_log_var = log_var\n","        self.current_recon = self.decode(z)\n","        \n","        return self.current_recon\n","\n","    def loss(self, *args, **kwargs) -> dict:\n","        self.num_iter += 1\n","        recons = self.current_recon\n","        input = self.current_inputs\n","        mu = self.current_mu\n","        log_var = self.current_log_var\n","        kld_weight = kwargs['kld_weight']  # Account for the minibatch samples from the dataset\n","        \n","        # since the image value is normalized between 0~1, BCE loss is better\n","        # recons_loss =F.binary_cross_entropy(recons, input)\n","        recons_loss = sum([F.mse_loss(recons, img) * (255 ** 2) for img in input])\n","  \n","        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n","\n","        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n","            loss = recons_loss + self.beta * kld_weight * kld_loss\n","        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n","            self.C_max = self.C_max.to(device) #input.device\n","            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n","            loss = recons_loss + self.gamma * kld_weight * (kld_loss - C).abs()\n","        else:\n","            raise ValueError('Undefined loss type.')\n","\n","        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}\n","\n","    def sample(self,\n","               num_samples:int,\n","               current_device: int, **kwargs) -> Tensor:\n","        \"\"\"\n","        Samples from the latent space and return the corresponding\n","        image space map.\n","        :param num_samples: (Int) Number of samples\n","        :param current_device: (Int) Device to run the model\n","        :return: (Tensor)\n","        \"\"\"\n","        z = torch.randn(num_samples,\n","                        self.latent_dim)\n","\n","        z = z.to(current_device)\n","\n","        samples = self.decode(z)\n","        return samples\n","\n","    def generate(self, x: Tensor, **kwargs) -> Tensor:\n","        \"\"\"\n","        Given an input image x, returns the reconstructed image\n","        :param x: (Tensor) [B x C x H x W]\n","        :return: (Tensor) [B x C x H x W]\n","        \"\"\"\n","\n","        return self.forward(x)[0]"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ip1VxfrZoar9"},"source":["### Resume from checkpoint or a new model?"]},{"cell_type":"markdown","metadata":{"id":"ve7PzstToar_"},"source":["#### train a new model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pAgeJrlcYk7","executionInfo":{"status":"ok","timestamp":1619862451112,"user_tz":-480,"elapsed":788,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}},"outputId":"ce362113-e8e9-4984-9e08-6b1ed15471ac"},"source":["import torchsummary\n","model_id = \"model_03\"\n","\n","model = BetaVAE(3, 32)\n","\n","epoch_start = 0\n","model.to(device)\n","print(model)\n","\n","# model_spec = torchsummary.summary_string(model, (3, 128, 128))[0]\n","# print(model_spec)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["BetaVAE(\n","  (encoder): Sequential(\n","    (0): Sequential(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (1): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (5): Flatten(start_dim=1, end_dim=-1)\n","  )\n","  (fc_mu): Linear(in_features=24576, out_features=32, bias=True)\n","  (fc_var): Linear(in_features=24576, out_features=32, bias=True)\n","  (decoder_input): Linear(in_features=32, out_features=8192, bias=True)\n","  (decoder): Sequential(\n","    (0): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (1): Sequential(\n","      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (2): Sequential(\n","      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","    (3): Sequential(\n","      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (final_layer): Sequential(\n","    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): LeakyReLU(negative_slope=0.01)\n","    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): Tanh()\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"USNbm5dfoasA","executionInfo":{"status":"ok","timestamp":1619859749281,"user_tz":-480,"elapsed":856,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["PATH = \"/content/gdrive/MyDrive/IDL 11785/project\"\n","\n","os.mkdir(f\"{PATH}/{model_store}/{model_id}\")\n","# save model summary to a txt file\n","with open(f\"{PATH}/{model_store}/{model_id}/model_spec.txt\", \"w\") as file:\n","  file.write(str(model) + \"\\n\")\n","  # file.write(model_spec)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vnEDh1EtoasB"},"source":["#### load a trained model from checkpoint "]},{"cell_type":"code","metadata":{"id":"vozrJJ1kKHdY","executionInfo":{"status":"ok","timestamp":1619859801587,"user_tz":-480,"elapsed":992,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["def load_model(model_id, specific_epoch = None):\n","  global optimizer, scheduler\n","  epoch_start = -1\n","  for checkpoint in os.listdir(f\"{PATH}/{model_store}/{model_id}\"):\n","    if not checkpoint.startswith(\"epoch\"):\n","      continue\n","    epoch = int(checkpoint.split(\"_\")[1])\n","    if specific_epoch is None:\n","      # find the latest\n","      if epoch > epoch_start:\n","        epoch_start = epoch\n","        last_checkpoint = checkpoint\n","    else:\n","      if epoch == specific_epoch:\n","        epoch_start = epoch\n","        last_checkpoint = checkpoint\n","        break\n","\n","  if epoch_start == -1:\n","    print(f\"No checkpoints available for {model_id}!\")\n","    return -1, None\n","  else:\n","    epoch_start += 1\n","    print(f\"resuming from last checkpoint {last_checkpoint}\")\n","    data = torch.load(f\"{model_store}/{model_id}/{last_checkpoint}\")\n","    \n","    model = data.model\n","    optimizer = data.optimizer\n","    scheduler = data.scheduler\n","    criterion = data.criterion\n","    \n","    model.to(device)\n","    return epoch_start, model, criterion"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyEuw_GWoasC"},"source":["model_id = \"model_03\"\n","epoch_start, model, criterion = load_model(model_id)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzPoXTvboasD"},"source":["### Start training"]},{"cell_type":"code","metadata":{"id":"_AJkfyLMoasD","executionInfo":{"status":"ok","timestamp":1619859817806,"user_tz":-480,"elapsed":834,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["# clear GPU cache\n","if has_cuda:\n","  torch.cuda.empty_cache()"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3Jw1cHCKHdZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619860788219,"user_tz":-480,"elapsed":840,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}},"outputId":"9290fa3d-69bf-43f8-c51f-7fe3ae890807"},"source":["train_dataloader_args = dict(batch_size=128,\n","                             num_workers=0 if is_windows else 4) if has_cuda else dict(batch_size=64)\n","train_dataloader_args[\"shuffle\"] = True\n","\n","train_dataloader = data.DataLoader(dataset, **train_dataloader_args)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  data = _utils.pin_memory.pin_memory(data)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CcFDowasoasE","executionInfo":{"status":"ok","timestamp":1619860831127,"user_tz":-480,"elapsed":1016,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}}},"source":["from torch import optim\n","from itertools import chain\n","\n","num_epochs = 100\n","\n","if epoch_start == 0:\n","  # define only at the start of the training\n","  \n","  regularization = 2e-5\n","#   learning_rate = 1e-1\n","#   optimizer = optim.SGD(chain(model.parameters(), criterion.parameters()),\n","#                          lr = learning_rate, momentum=0.9, weight_decay=regularization, nesterov=True)\n","  learning_rate = 1e-3\n","  optimizer = optim.Adam(model.parameters(),\n","                         lr = learning_rate, weight_decay=regularization)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.5)\n","\n","# scaler = torch.cuda.amp.GradScaler() # mix-precision training\n","\n","with open(f\"/content/gdrive/MyDrive/IDL 11785/project/{model_store}/{model_id}/training_params.txt\", \"w\") as file:\n","  file.write(f\"num_epochs = {num_epochs}\\n\")\n","  file.write(f\"optimizer = {optimizer}\\n\")\n","  file.write(f\"scheduler = {type(scheduler).__name__}({scheduler.state_dict()})\\n\")"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"7wRgUFh2fGOa","executionInfo":{"status":"error","timestamp":1619864447117,"user_tz":-480,"elapsed":792959,"user":{"displayName":"Yinghao Ma","photoUrl":"","userId":"06584988386168278880"}},"outputId":"1836d9df-2661-408f-c2c4-56956d622b39"},"source":["from tqdm import tqdm\n","import sys\n","import json\n","\n","print(f\"Model: {model_id}. Training for {num_epochs} epochs\", file=sys.stderr)\n","\n","for epoch in tqdm(range(epoch_start, num_epochs), desc=\"Train\"):\n","  print(f\"Epoch {epoch}\", file=sys.stderr)\n","  \n","  # set model in training mode\n","  model.train()\n","  training_loss = 0.0\n","  reconstruction_loss = 0.0\n","  kld_loss = 0.0\n","\n","  for x in train_dataloader:\n","    optimizer.zero_grad() # clear calculated gradients\n","\n","    x = [i.to(device) for i in x]\n","    \n","    with torch.cuda.amp.autocast():\n","      output = model(x)\n","      all_loss = model.loss(kld_weight=1.0)\n","      loss = all_loss[\"loss\"]\n","    \n","    # backpropo loss and accumuate loss stat\n","    # scaler.scale(loss).backward()    \n","    \n","    training_loss += loss.detach().item() # otherwise this would be a tensor\n","    reconstruction_loss += all_loss['Reconstruction_Loss'].detach().item()\n","    kld_loss += all_loss['KLD'].detach().item()\n","\n","    # scaler.step(optimizer)\n","    # scaler.update()\n","    loss.backward()\n","    optimizer.step()\n","    \n","  # let scheduler know it's the next epoch\n","  scheduler.step()\n","  \n","  training_loss /= len(train_dataloader)\n","  reconstruction_loss /= len(train_dataloader)\n","  kld_loss /= len(train_dataloader)\n","  \n","  log_str = json.dumps({\n","    \"Epoch\": epoch,\n","    \"training loss\": round(training_loss, 6),\n","    \"reconstruction loss\": round(reconstruction_loss, 6),\n","    \"KLD loss\": round(kld_loss, 6),\n","    \"Learning rate\": scheduler._last_lr\n","  })\n","\n","#   log_str = f\"Epoch {epoch}: training loss {training_loss:.6f}, \" +\\\n","#             f\"reconstruction loss {reconstruction_loss:.6f}, kld_loss {kld_loss:.6f}\"+\\\n","#             f\" Learning Rate: {scheduler._last_lr}\"\n"," \n","  with open(f\"{model_store}/{model_id}/training_logs.txt\", \"a\") as log_file:\n","    log_file.write(log_str + \"\\n\")\n","  print(log_str, file=sys.stderr)\n","  \n","  torch.save(StoredModel(model, optimizer, scheduler, None),\n","             f\"{model_store}/{model_id}/epoch_{epoch:02d}\" +\\\n","             f\"_tr-loss_{training_loss:.6f}\")"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Model: model_03. Training for 100 epochs\n","\n","\n","\n","\n","\n","Train:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[AEpoch 0\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  data = _utils.pin_memory.pin_memory(data)\n"],"name":"stderr"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-779d534bcbb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#             f\" Learning Rate: {scheduler._last_lr}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_store}/{model_id}/training_logs.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_checkpoints/model_03/training_logs.txt'"]}]},{"cell_type":"code","metadata":{"id":"c9rpNw9ZoasF","outputId":"5e8ce878-9137-48e7-a6b4-040f83454fbb"},"source":["from tqdm import tqdm\n","\n","validataion_dataloader_args = dict(batch_size=128,\n","                             num_workers=0 if is_windows else 2) if has_cuda else dict(batch_size=64)\n","validataion_dataloader_args[\"shuffle\"] = False\n","\n","validataion_dataloader = data.DataLoader(dataset, **validataion_dataloader_args)\n","\n","# set model in training mode\n","model.eval()\n","\n","latent_mu = list()\n","latent_log_var = list()\n","\n","for i, x in enumerate(tqdm(validataion_dataloader, desc=\"Validate\")):\n","  x = x.to(device)\n","\n","  _, mus, log_vars = model.encode(x)\n","  latent_mu.append(mus.detach().cpu())\n","  latent_log_var.append(log_vars.detach().cpu())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Validate: 100%|██████████| 1500/1500 [41:09<00:00,  1.65s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8iiXKRd6oasF"},"source":["torch.save((torch.vstack(latent_mu), torch.vstack(latent_log_var)), f\"latent_vectors/{model_id}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNbtdGmCoasG","outputId":"6f02fe17-8991-4528-c6a1-d7818e0f86a6"},"source":["L2_divergence_raw = list()\n","\n","image_1 = dataset[0].to(device)\n","\n","for i in tqdm(range(len(dataset) - 1), desc=\"L2\"):\n","  image_2 = dataset[i + 1].to(device)\n","  \n","  diff = (image_1 - image_2).flatten()\n","  \n","  L2_divergence_raw.append(torch.linalg.norm(diff, 2).cpu().item())\n","  \n","  image_1 = image_2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["L2: 100%|██████████| 191880/191880 [48:40<00:00, 65.69it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"MSqk_96uoasG"},"source":["torch.save(torch.tensor(L2_divergence_raw), f\"temp_store/{model_id}/l2_divergence_raw\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XouHiYfkoasH","outputId":"ff7d940f-cf72-4a67-d608-2f649476fdb8"},"source":["normalize = lambda X, mn, mx: [(x - mn)/(mx - mn) for x in X]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63.88896179199219"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"Nmd6yOEkoasH"},"source":[""],"execution_count":null,"outputs":[]}]}