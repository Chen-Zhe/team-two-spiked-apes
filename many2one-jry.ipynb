{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1FWITFR4kdt"
   },
   "source": [
    "# Many to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMrhEjZQNpaf"
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x0fv3h9wgVU"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1619860781169,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "UVfVlcXvck7x",
    "outputId": "1cfb1475-b248-4438-a82e-2ff5e14be7b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision as vis\n",
    "import sys\n",
    "\n",
    "is_windows = sys.platform == \"win32\"\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if has_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i6Bo8EOtoar2"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "class FlatImageData(vis.datasets.VisionDataset):\n",
    "  def __init__(self, root, transform, validation_reserved_images=31136, win_len=10):\n",
    "    self.root = root\n",
    "    self.images = os.listdir(root)\n",
    "    self.images.sort(key=lambda x: int(x[6:-5]))# sort by frame no.\n",
    "    self.transform = transform\n",
    "    self.training_mode = True\n",
    "    self.reserved_images = validation_reserved_images\n",
    "    self.win_len = win_len\n",
    "        \n",
    "  # number of windows available\n",
    "  def __len__(self):\n",
    "    if self.training_mode:\n",
    "      return (len(self.images) - self.reserved_images) // (self.win_len - 1)\n",
    "    else:\n",
    "      return self.reserved_images // (self.win_len - 1)\n",
    "    \n",
    "  def pil_loader(self, path: str) -> Image.Image:\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "  # load the ith window of shape (win_len, C, H, W)\n",
    "  # each window has a 1 overlap with adjacent window so that we don't miss a change\n",
    "  def __getitem__(self, index):\n",
    "    index *= (self.win_len - 1)\n",
    "    if self.training_mode:\n",
    "      index += self.reserved_images\n",
    "    \n",
    "    image_name = [self.images[index+i] for i in range(self.win_len)]\n",
    "    image_path = [f\"{self.root}/{i}\" for i in image_name]\n",
    "    image = [self.pil_loader(img) for img in image_path]\n",
    "    if self.transform is not None:\n",
    "         image = [self.transform(img) for img in image]\n",
    "    return torch.stack(image)\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "      return torch.stack(batch)\n",
    "\n",
    "\n",
    "class FlatSingleImageData(vis.datasets.VisionDataset):\n",
    "  def __init__(self, root, transform, validation_reserved_images=31136):\n",
    "    self.root = root\n",
    "    self.images = os.listdir(root)\n",
    "    self.images.sort(key=lambda x: int(x[6:-5]))# sort by frame no.\n",
    "    self.transform = transform\n",
    "    self.training_mode = True\n",
    "    self.reserved_images = validation_reserved_images\n",
    "        \n",
    "  def __len__(self):\n",
    "    if self.training_mode:\n",
    "      return len(self.images) - self.reserved_images\n",
    "    else:\n",
    "      return self.reserved_images\n",
    "    \n",
    "  def pil_loader(self, path: str) -> Image.Image:\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    if self.training_mode:\n",
    "      index += self.reserved_images\n",
    "    \n",
    "    image_name = self.images[index]\n",
    "    image_path = f\"{self.root}/{image_name}\"\n",
    "    image = self.pil_loader(image_path)\n",
    "    if self.transform is not None:\n",
    "         image = self.transform(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "      return torch.stack(batch).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1619860686396,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "A6Iqt5YAoar3",
    "outputId": "7aea0d6f-a656-41ed-c146-d82014c48f2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FlatImageData\n",
       "    Number of datapoints: 17860\n",
       "    Root location: knnw-720p"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_DATA = \"knnw-720p\"\n",
    "dataset = FlatImageData(root=PATH_DATA, \n",
    "                             transform=vis.transforms.Compose([\n",
    "                               vis.transforms.RandomHorizontalFlip(),\n",
    "                               vis.transforms.RandomApply(nn.ModuleList([\n",
    "                                 vis.transforms.RandomAffine(degrees=15),\n",
    "                                 vis.transforms.CenterCrop((1024, 576))\n",
    "                               ]), p=0.5),\n",
    "                               vis.transforms.ToTensor(),\n",
    "                               nn.AdaptiveAvgPool2d((128, 128))\n",
    "                             ])\n",
    "                            )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjCIyjpWdqGw"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "52LRNqjIoar4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_store = \"model_checkpoints\"\n",
    "\n",
    "class StoredModel:\n",
    "  def __init__(self, model, optimizer, scheduler, criterion):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.scheduler = scheduler\n",
    "    self.criterion = criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wo1FBitaoar4"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "\n",
    "    num_iter = 0 # Global static variable to keep track of iterations\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 beta: int = 4,\n",
    "                 gamma:float = 1000.,\n",
    "                 max_capacity: int = 25,\n",
    "                 Capacity_max_iter: int = 1e5,\n",
    "                 loss_type:str = 'B',\n",
    "                 **kwargs) -> None:\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.loss_type = loss_type\n",
    "        self.C_max = torch.Tensor([max_capacity])\n",
    "        self.C_stop_iter = Capacity_max_iter\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "            \n",
    "        modules.append(nn.Flatten())\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*16, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*16, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 16)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1))\n",
    "\n",
    "    def encode(self, inputs: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x win_len x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        batch_size, window_size, C, H, W = inputs.shape\n",
    "        result = self.encoder(inputs.view(-1, C, H, W))\n",
    "        combined_features = result.view(batch_size, window_size, -1).mean(dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(combined_features)\n",
    "        log_var = self.fc_var(combined_features)\n",
    "\n",
    "        return (inputs, mu, log_var)\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 4, 4)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Will a single z be enough ti compute the expectation\n",
    "        for the loss??\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, inputs: Tensor, **kwargs) -> Tensor:\n",
    "        pooled_inputs, mu, log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        self.current_inputs = pooled_inputs\n",
    "        self.current_mu = mu\n",
    "        self.current_log_var = log_var\n",
    "        self.current_recon = self.decode(z)\n",
    "        \n",
    "        return self.current_recon\n",
    "\n",
    "    def loss(self, *args, **kwargs) -> dict:\n",
    "        self.num_iter += 1\n",
    "        recons = self.current_recon\n",
    "        input = self.current_inputs\n",
    "        mu = self.current_mu\n",
    "        log_var = self.current_log_var\n",
    "        kld_weight = kwargs['kld_weight']  # Account for the minibatch samples from the dataset\n",
    "        \n",
    "        # since the image value is normalized between 0~1, BCE loss is better\n",
    "        batch_size = recons.shape[0]\n",
    "        recons_loss =F.binary_cross_entropy_with_logits(recons, input.mean(dim=1), reduction='sum') / batch_size\n",
    "        # recons_loss = sum([F.mse_loss(recons, img) * (255 ** 2) for img in input])\n",
    "  \n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            loss = recons_loss + self.beta * kld_weight * kld_loss\n",
    "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
    "            self.C_max = self.C_max.to(device) #input.device\n",
    "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
    "            loss = recons_loss + self.gamma * kld_weight * (kld_loss - C).abs()\n",
    "        else:\n",
    "            raise ValueError('Undefined loss type.')\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss': recons_loss, 'KLD':kld_loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip1VxfrZoar9"
   },
   "source": [
    "### Resume from checkpoint or a new model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve7PzstToar_"
   },
   "source": [
    "#### train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1619862451112,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "-pAgeJrlcYk7",
    "outputId": "ce362113-e8e9-4984-9e08-6b1ed15471ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (fc_var): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (decoder_input): Linear(in_features=32, out_features=8192, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "model_id = \"manytoone_BCE_B_loss\"\n",
    "\n",
    "model = BetaVAE(3, 32)\n",
    "\n",
    "epoch_start = 0\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "#model_spec = torchsummary.summary_string(model, (3, 128, 128))[0]\n",
    "#print(model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "USNbm5dfoasA"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'model_checkpoints/manytoone_BCE_B_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f6138871171f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_store}/{model_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# save model summary to a txt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#with open(f\"{model_store}/{model_id}/model_spec.txt\", \"w\") as file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#file.write(str(model) + \"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#file.write(model_spec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'model_checkpoints/manytoone_BCE_B_loss'"
     ]
    }
   ],
   "source": [
    "os.mkdir(f\"{model_store}/{model_id}\")\n",
    "# save model summary to a txt file\n",
    "#with open(f\"{model_store}/{model_id}/model_spec.txt\", \"w\") as file:\n",
    "  #file.write(str(model) + \"\\n\")\n",
    "  #file.write(model_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnEDh1EtoasB"
   },
   "source": [
    "#### load a trained model from checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vozrJJ1kKHdY"
   },
   "outputs": [],
   "source": [
    "def load_model(model_id, specific_epoch = None):\n",
    "  global optimizer, scheduler\n",
    "  epoch_start = -1\n",
    "  for checkpoint in os.listdir(f\"{model_store}/{model_id}\"):\n",
    "    if not checkpoint.startswith(\"epoch\"):\n",
    "      continue\n",
    "    epoch = int(checkpoint.split(\"_\")[1])\n",
    "    if specific_epoch is None:\n",
    "      # find the latest\n",
    "      if epoch > epoch_start:\n",
    "        epoch_start = epoch\n",
    "        last_checkpoint = checkpoint\n",
    "    else:\n",
    "      if epoch == specific_epoch:\n",
    "        epoch_start = epoch\n",
    "        last_checkpoint = checkpoint\n",
    "        break\n",
    "\n",
    "  if epoch_start == -1:\n",
    "    print(f\"No checkpoints available for {model_id}!\")\n",
    "    return -1, None\n",
    "  else:\n",
    "    epoch_start += 1\n",
    "    print(f\"resuming from last checkpoint {last_checkpoint}\")\n",
    "    data = torch.load(f\"{model_store}/{model_id}/{last_checkpoint}\")\n",
    "    \n",
    "    model = data.model\n",
    "    optimizer = data.optimizer\n",
    "    scheduler = data.scheduler\n",
    "    criterion = data.criterion\n",
    "    \n",
    "    model.to(device)\n",
    "    return epoch_start, model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uyEuw_GWoasC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming from last checkpoint epoch_19_tr-loss_28362.519169\n",
      "BetaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (fc_var): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (decoder_input): Linear(in_features=32, out_features=8192, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"manytoone_BCE_B_loss\"\n",
    "epoch_start, model, criterion = load_model(model_id)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzPoXTvboasD"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_AJkfyLMoasD"
   },
   "outputs": [],
   "source": [
    "# clear GPU cache\n",
    "if has_cuda:\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1619860788219,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "S3Jw1cHCKHdZ",
    "outputId": "9290fa3d-69bf-43f8-c51f-7fe3ae890807"
   },
   "outputs": [],
   "source": [
    "train_dataloader_args = dict(batch_size=128,\n",
    "                             num_workers=0 if is_windows else 4) if has_cuda else dict(batch_size=64)\n",
    "train_dataloader_args[\"shuffle\"] = True\n",
    "train_dataloader_args[\"collate_fn\"] = dataset.collate_fn\n",
    "\n",
    "train_dataloader = data.DataLoader(dataset, **train_dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CcFDowasoasE"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from itertools import chain\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "if epoch_start == 0:\n",
    "  # define only at the start of the training\n",
    "  \n",
    "  regularization = 2e-5\n",
    "#   learning_rate = 1e-1\n",
    "#   optimizer = optim.SGD(chain(model.parameters(), criterion.parameters()),\n",
    "#                          lr = learning_rate, momentum=0.9, weight_decay=regularization, nesterov=True)\n",
    "  learning_rate = 1e-3\n",
    "  optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=regularization)\n",
    "  #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.5)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', threshold=0.001, factor=0.5, patience=3)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() # mix-precision training\n",
    "\n",
    "with open(f\"{model_store}/{model_id}/training_params.txt\", \"w\") as file:\n",
    "  file.write(f\"num_epochs = {num_epochs}\\n\")\n",
    "  file.write(f\"optimizer = {optimizer}\\n\")\n",
    "  file.write(f\"scheduler = {type(scheduler).__name__}({scheduler.state_dict()})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually increase learning rate\n",
    "#for param_group in optimizer.param_groups:\n",
    "#  param_group['lr'] = 1e-3\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', threshold=0.001, factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 792959,
     "status": "error",
     "timestamp": 1619864447117,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "7wRgUFh2fGOa",
    "outputId": "1836d9df-2661-408f-c2c4-56956d622b39"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import json\n",
    "\n",
    "print(f\"Model: {model_id}. Training for {num_epochs} epochs\", file=sys.stderr)\n",
    "\n",
    "for epoch in range(epoch_start, num_epochs):\n",
    "  print(f\"Epoch {epoch}\", file=sys.stderr)\n",
    "  \n",
    "  # set model in training mode\n",
    "  model.train()\n",
    "  training_loss = 0.0\n",
    "  reconstruction_loss = 0.0\n",
    "  kld_loss = 0.0\n",
    "\n",
    "  for x in tqdm(train_dataloader, desc=\"Train\"):\n",
    "    optimizer.zero_grad() # clear calculated gradients\n",
    "\n",
    "    x = x.to(device)\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "      output = model(x)\n",
    "      all_loss = model.loss(kld_weight=1.0)\n",
    "      loss = all_loss[\"loss\"]\n",
    "    \n",
    "    # backpropo loss and accumuate loss stat\n",
    "    scaler.scale(loss).backward()    \n",
    "    \n",
    "    training_loss += loss.detach().item() # otherwise this would be a tensor\n",
    "    reconstruction_loss += all_loss['Reconstruction_Loss'].detach().item()\n",
    "    kld_loss += all_loss['KLD'].detach().item()\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "  # let scheduler know it's the next epoch\n",
    "  training_loss /= len(train_dataloader)\n",
    "  reconstruction_loss /= len(train_dataloader)\n",
    "  kld_loss /= len(train_dataloader)\n",
    "    \n",
    "  scheduler.step(training_loss)\n",
    "  \n",
    "  log_str = json.dumps({\n",
    "    \"Epoch\": epoch,\n",
    "    \"training loss\": round(training_loss, 6),\n",
    "    \"reconstruction loss\": round(reconstruction_loss, 6),\n",
    "    \"KLD loss\": round(kld_loss, 6),\n",
    "    \"Learning rate\": scheduler._last_lr\n",
    "  })\n",
    "\n",
    "#   log_str = f\"Epoch {epoch}: training loss {training_loss:.6f}, \" +\\\n",
    "#             f\"reconstruction loss {reconstruction_loss:.6f}, kld_loss {kld_loss:.6f}\"+\\\n",
    "#             f\" Learning Rate: {scheduler._last_lr}\"\n",
    " \n",
    "  with open(f\"{model_store}/{model_id}/training_logs.txt\", \"a\") as log_file:\n",
    "    log_file.write(log_str + \"\\n\")\n",
    "  print(log_str, file=sys.stderr)\n",
    "  \n",
    "  torch.save(StoredModel(model, optimizer, scheduler, None),\n",
    "             f\"{model_store}/{model_id}/epoch_{epoch:02d}\" +\\\n",
    "             f\"_tr-loss_{training_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c9rpNw9ZoasF",
    "outputId": "5e8ce878-9137-48e7-a6b4-040f83454fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validate: 100%|██████████| 244/244 [04:44<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# calculate latent vectors of validation frames\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "validation_dataset = FlatSingleImageData(root=\"knnw-720p\",\n",
    "                             transform=vis.transforms.Compose([\n",
    "                               vis.transforms.ToTensor(),\n",
    "                               nn.AdaptiveAvgPool2d((128, 128))\n",
    "                             ])\n",
    "                       )\n",
    "\n",
    "validation_dataset.training_mode = False\n",
    "\n",
    "validation_dataloader_args = dict(batch_size=128,\n",
    "                             num_workers=0 if is_windows else 2) if has_cuda else dict(batch_size=64)\n",
    "validation_dataloader_args[\"shuffle\"] = False\n",
    "validation_dataloader_args[\"collate_fn\"] = validation_dataset.collate_fn\n",
    "\n",
    "validation_dataloader = data.DataLoader(validation_dataset, **validation_dataloader_args)\n",
    "\n",
    "# set model in training mode\n",
    "model.eval()\n",
    "\n",
    "latent_mu = list()\n",
    "latent_log_var = list()\n",
    "\n",
    "for i, x in enumerate(tqdm(validation_dataloader, desc=\"Validate\")):\n",
    "  x = x.to(device)\n",
    "\n",
    "  _, mus, log_vars = model.encode(x)\n",
    "  latent_mu.append(mus.detach().cpu())\n",
    "  latent_log_var.append(log_vars.detach().cpu())\n",
    "\n",
    "latent_mu = torch.cat(latent_mu, dim=0)\n",
    "latent_log_var = torch.cat(latent_log_var, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L2: 100%|██████████| 31135/31135 [00:01<00:00, 20751.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# get L2 distances of adjacent validation frame pairs\n",
    "\n",
    "L2_divergence_latent = list()\n",
    "\n",
    "for i in tqdm(range(31136 - 1), desc=\"L2\"):\n",
    "  image_1 = torch.cat((latent_mu[i], torch.exp(latent_log_var[i]).sqrt()))\n",
    "  image_2 = torch.cat((latent_mu[i + 1], torch.exp(latent_log_var[i + 1]).sqrt()))\n",
    "  \n",
    "  diff = (image_1 - image_2).flatten()\n",
    "  \n",
    "  L2_divergence_latent.append(torch.linalg.norm(diff, 2).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8iiXKRd6oasF"
   },
   "outputs": [],
   "source": [
    "# calculate the L2 distance of labelled scene changes as thresholds for calculating AUC\n",
    "# and their indices\n",
    "\n",
    "import numpy as np\n",
    "from math import floor\n",
    "scene_changes = []\n",
    "with open(\"scene-change.csv\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        scenes = line.split(',')\n",
    "        scene_changes.append([floor(int(scenes[0][6:12])/25*30+25), floor(int(scenes[1][6:12])/25*30+32)])\n",
    "\n",
    "L2_divergence_latent = np.array(L2_divergence_latent)\n",
    "\n",
    "max_l2_per_scene_change = list()\n",
    "scene_change_ids = list()\n",
    "\n",
    "for start_id, end_id in scene_changes:\n",
    "  start_id -= 1\n",
    "  max_l2_per_scene_change.append(max(L2_divergence_latent[start_id: end_id]))\n",
    "  scene_change_ids.append(L2_divergence_latent[start_id: end_id].argmax() + start_id)\n",
    "  \n",
    "max_l2_per_scene_change = np.array(sorted(max_l2_per_scene_change, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate AUC\n",
    "# correct means we use the ith largest L2 distance in labelled scene changes as threshold\n",
    "# total means the number of scene changes we will get \n",
    "\n",
    "correct = list()\n",
    "total = list()\n",
    "\n",
    "for i, divergence in enumerate(max_l2_per_scene_change):\n",
    "  correct.append(i + 1)\n",
    "  total.append((L2_divergence_latent >= divergence).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zNbtdGmCoasG",
    "outputId": "6f02fe17-8991-4528-c6a1-d7818e0f86a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe636633250>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 261,
       "width": 388
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_ylabel('#Scene Changed Frames')\n",
    "ax1.set_xlabel('#Frames above Threshold')\n",
    "plt.plot(total, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSqk_96uoasG"
   },
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(L2_divergence_raw), f\"temp_store/{model_id}/l2_divergence_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XouHiYfkoasH",
    "outputId": "ff7d940f-cf72-4a67-d608-2f649476fdb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.88896179199219"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = lambda X, mn, mx: [(x - mn)/(mx - mn) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmd6yOEkoasH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "many2one-jry.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_11785)",
   "language": "python",
   "name": "conda_11785"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
