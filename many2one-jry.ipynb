{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1FWITFR4kdt"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMrhEjZQNpaf"
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x0fv3h9wgVU"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1619860781169,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "UVfVlcXvck7x",
    "outputId": "1cfb1475-b248-4438-a82e-2ff5e14be7b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision as vis\n",
    "import sys\n",
    "\n",
    "is_windows = sys.platform == \"win32\"\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if has_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i6Bo8EOtoar2"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class FlatImageData(vis.datasets.VisionDataset):\n",
    "  def __init__(self, root, transform, validation_reserved_images=31136, win_len=10):\n",
    "    self.root = root\n",
    "    self.images = os.listdir(root)\n",
    "    self.images.sort(key=lambda x: int(x[6:-5]))# sort by frame no.\n",
    "    self.transform = transform\n",
    "    self.training_mode = True\n",
    "    self.reserved_images = validation_reserved_images\n",
    "    self.win_len = win_len\n",
    "        \n",
    "  # number of windows available\n",
    "  def __len__(self):\n",
    "    if self.training_mode:\n",
    "      return (len(self.images) - self.reserved_images) // (self.win_len - 1)\n",
    "    else:\n",
    "      return self.reserved_images // (self.win_len - 1)\n",
    "    \n",
    "  def pil_loader(self, path: str) -> Image.Image:\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "  # load the ith window of shape (win_len, C, H, W)\n",
    "  # each window has a 1 overlap with adjacent window so that we don't miss a change\n",
    "  def __getitem__(self, index):\n",
    "    index *= (self.win_len - 1)\n",
    "    if self.training_mode:\n",
    "      index += self.reserved_images\n",
    "    \n",
    "    image_name = [self.images[index+i] for i in range(self.win_len)]\n",
    "    image_path = [f\"{self.root}/{i}\" for i in image_name]\n",
    "    image = [self.pil_loader(img) for img in image_path]\n",
    "    if self.transform is not None:\n",
    "         image = [self.transform(img) for img in image]\n",
    "    return torch.stack(image)\n",
    "\n",
    "  def collate_fn(batch):\n",
    "      return torch.as_tensor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1619860686396,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "A6Iqt5YAoar3",
    "outputId": "7aea0d6f-a656-41ed-c146-d82014c48f2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FlatImageData\n",
       "    Number of datapoints: 17860\n",
       "    Root location: knnw-720p"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_DATA = \"knnw-720p\"\n",
    "dataset = FlatImageData(root=PATH_DATA, \n",
    "                             transform=vis.transforms.Compose([\n",
    "                               vis.transforms.RandomHorizontalFlip(),\n",
    "                               vis.transforms.RandomApply(nn.ModuleList([\n",
    "                                 vis.transforms.RandomAffine(degrees=15),\n",
    "                                 vis.transforms.CenterCrop((1024, 576))\n",
    "                               ]), p=0.5),\n",
    "                               vis.transforms.ToTensor(),\n",
    "                               nn.AdaptiveAvgPool2d((128, 128))\n",
    "                             ])\n",
    "                            )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjCIyjpWdqGw"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "52LRNqjIoar4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "model_store = \"model_checkpoints\"\n",
    "\n",
    "class StoredModel:\n",
    "  def __init__(self, model, optimizer, scheduler, criterion):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.scheduler = scheduler\n",
    "    self.criterion = criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wo1FBitaoar4"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "\n",
    "    num_iter = 0 # Global static variable to keep track of iterations\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 beta: int = 4,\n",
    "                 gamma:float = 1000.,\n",
    "                 max_capacity: int = 25,\n",
    "                 Capacity_max_iter: int = 1e5,\n",
    "                 loss_type:str = 'B',\n",
    "                 **kwargs) -> None:\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.loss_type = loss_type\n",
    "        self.C_max = torch.Tensor([max_capacity])\n",
    "        self.C_stop_iter = Capacity_max_iter\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "            \n",
    "        modules.append(nn.Flatten())\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*16, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*16, latent_dim)\n",
    "\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 16)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1))\n",
    "\n",
    "    def encode(self, inputs: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x win_len x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        batch_size, window_size, C, H, W = inputs.shape\n",
    "        result = self.encoder(inputs.view(-1, C, H, W))\n",
    "        combined_features = result.view(batch_size, window_size, -1).mean(dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(combined_features)\n",
    "        log_var = self.fc_var(combined_features)\n",
    "\n",
    "        return (inputs, mu, log_var)\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 4, 4)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Will a single z be enough ti compute the expectation\n",
    "        for the loss??\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, inputs: Tensor, **kwargs) -> Tensor:\n",
    "        pooled_inputs, mu, log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        self.current_inputs = pooled_inputs\n",
    "        self.current_mu = mu\n",
    "        self.current_log_var = log_var\n",
    "        self.current_recon = self.decode(z)\n",
    "        \n",
    "        return self.current_recon\n",
    "\n",
    "    def loss(self, *args, **kwargs) -> dict:\n",
    "        self.num_iter += 1\n",
    "        recons = self.current_recon\n",
    "        input = self.current_inputs\n",
    "        mu = self.current_mu\n",
    "        log_var = self.current_log_var\n",
    "        kld_weight = kwargs['kld_weight']  # Account for the minibatch samples from the dataset\n",
    "        \n",
    "        # since the image value is normalized between 0~1, BCE loss is better\n",
    "        batch_size = recons.shape[0]\n",
    "        recons_loss =F.binary_cross_entropy_with_logits(recons, input.mean(dim=1), reduction='sum') / batch_size\n",
    "        # recons_loss = sum([F.mse_loss(recons, img) * (255 ** 2) for img in input])\n",
    "  \n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        if self.loss_type == 'H': # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            loss = recons_loss + self.beta * kld_weight * kld_loss\n",
    "        elif self.loss_type == 'B': # https://arxiv.org/pdf/1804.03599.pdf\n",
    "            self.C_max = self.C_max.to(device) #input.device\n",
    "            C = torch.clamp(self.C_max/self.C_stop_iter * self.num_iter, 0, self.C_max.data[0])\n",
    "            loss = recons_loss + self.gamma * kld_weight * (kld_loss - C).abs()\n",
    "        else:\n",
    "            raise ValueError('Undefined loss type.')\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss': recons_loss, 'KLD':kld_loss}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip1VxfrZoar9"
   },
   "source": [
    "### Resume from checkpoint or a new model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve7PzstToar_"
   },
   "source": [
    "#### train a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1619862451112,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "-pAgeJrlcYk7",
    "outputId": "ce362113-e8e9-4984-9e08-6b1ed15471ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (fc_var): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (decoder_input): Linear(in_features=32, out_features=8192, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "model_id = \"manytoone_BCE_B_loss\"\n",
    "\n",
    "model = BetaVAE(3, 32)\n",
    "\n",
    "epoch_start = 0\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "#model_spec = torchsummary.summary_string(model, (3, 128, 128))[0]\n",
    "#print(model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "USNbm5dfoasA"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'model_checkpoints/manytoone_BCE_B_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f6138871171f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_store}/{model_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# save model summary to a txt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#with open(f\"{model_store}/{model_id}/model_spec.txt\", \"w\") as file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#file.write(str(model) + \"\\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#file.write(model_spec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'model_checkpoints/manytoone_BCE_B_loss'"
     ]
    }
   ],
   "source": [
    "os.mkdir(f\"{model_store}/{model_id}\")\n",
    "# save model summary to a txt file\n",
    "#with open(f\"{model_store}/{model_id}/model_spec.txt\", \"w\") as file:\n",
    "  #file.write(str(model) + \"\\n\")\n",
    "  #file.write(model_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnEDh1EtoasB"
   },
   "source": [
    "#### load a trained model from checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vozrJJ1kKHdY"
   },
   "outputs": [],
   "source": [
    "def load_model(model_id, specific_epoch = None):\n",
    "  global optimizer, scheduler\n",
    "  epoch_start = -1\n",
    "  for checkpoint in os.listdir(f\"{model_store}/{model_id}\"):\n",
    "    if not checkpoint.startswith(\"epoch\"):\n",
    "      continue\n",
    "    epoch = int(checkpoint.split(\"_\")[1])\n",
    "    if specific_epoch is None:\n",
    "      # find the latest\n",
    "      if epoch > epoch_start:\n",
    "        epoch_start = epoch\n",
    "        last_checkpoint = checkpoint\n",
    "    else:\n",
    "      if epoch == specific_epoch:\n",
    "        epoch_start = epoch\n",
    "        last_checkpoint = checkpoint\n",
    "        break\n",
    "\n",
    "  if epoch_start == -1:\n",
    "    print(f\"No checkpoints available for {model_id}!\")\n",
    "    return -1, None\n",
    "  else:\n",
    "    epoch_start += 1\n",
    "    print(f\"resuming from last checkpoint {last_checkpoint}\")\n",
    "    data = torch.load(f\"{model_store}/{model_id}/{last_checkpoint}\")\n",
    "    \n",
    "    model = data.model\n",
    "    optimizer = data.optimizer\n",
    "    scheduler = data.scheduler\n",
    "    criterion = data.criterion\n",
    "    \n",
    "    model.to(device)\n",
    "    return epoch_start, model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uyEuw_GWoasC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming from last checkpoint epoch_12_tr-loss_28664.565402\n",
      "BetaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (fc_var): Linear(in_features=8192, out_features=32, bias=True)\n",
      "  (decoder_input): Linear(in_features=32, out_features=8192, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"manytoone_BCE_B_loss\"\n",
    "epoch_start, model, criterion = load_model(model_id)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzPoXTvboasD"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_AJkfyLMoasD"
   },
   "outputs": [],
   "source": [
    "# clear GPU cache\n",
    "if has_cuda:\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1619860788219,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "S3Jw1cHCKHdZ",
    "outputId": "9290fa3d-69bf-43f8-c51f-7fe3ae890807"
   },
   "outputs": [],
   "source": [
    "train_dataloader_args = dict(batch_size=128,\n",
    "                             num_workers=0 if is_windows else 4) if has_cuda else dict(batch_size=64)\n",
    "train_dataloader_args[\"shuffle\"] = True\n",
    "\n",
    "train_dataloader = data.DataLoader(dataset, **train_dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CcFDowasoasE"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from itertools import chain\n",
    "\n",
    "num_epochs = 40\n",
    "\n",
    "if epoch_start == 0:\n",
    "  # define only at the start of the training\n",
    "  \n",
    "  regularization = 2e-5\n",
    "#   learning_rate = 1e-1\n",
    "#   optimizer = optim.SGD(chain(model.parameters(), criterion.parameters()),\n",
    "#                          lr = learning_rate, momentum=0.9, weight_decay=regularization, nesterov=True)\n",
    "  learning_rate = 1e-3\n",
    "  optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=regularization)\n",
    "  #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.5)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', threshold=50, factor=0.5, patience=3)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler() # mix-precision training\n",
    "\n",
    "with open(f\"{model_store}/{model_id}/training_params.txt\", \"w\") as file:\n",
    "  file.write(f\"num_epochs = {num_epochs}\\n\")\n",
    "  file.write(f\"optimizer = {optimizer}\\n\")\n",
    "  file.write(f\"scheduler = {type(scheduler).__name__}({scheduler.state_dict()})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually increase learning rate\n",
    "#for param_group in optimizer.param_groups:\n",
    "#  param_group['lr'] = 1e-3\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', threshold=50, factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 792959,
     "status": "error",
     "timestamp": 1619864447117,
     "user": {
      "displayName": "Yinghao Ma",
      "photoUrl": "",
      "userId": "06584988386168278880"
     },
     "user_tz": -480
    },
    "id": "7wRgUFh2fGOa",
    "outputId": "1836d9df-2661-408f-c2c4-56956d622b39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model: manytoone_BCE_B_loss. Training for 40 epochs\n",
      "Epoch 13\n",
      "Train:  29%|██▊       | 40/140 [06:46<09:42,  5.82s/it] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import json\n",
    "\n",
    "print(f\"Model: {model_id}. Training for {num_epochs} epochs\", file=sys.stderr)\n",
    "\n",
    "for epoch in range(epoch_start, num_epochs):\n",
    "  print(f\"Epoch {epoch}\", file=sys.stderr)\n",
    "  \n",
    "  # set model in training mode\n",
    "  model.train()\n",
    "  training_loss = 0.0\n",
    "  reconstruction_loss = 0.0\n",
    "  kld_loss = 0.0\n",
    "\n",
    "  for x in tqdm(train_dataloader, desc=\"Train\"):\n",
    "    optimizer.zero_grad() # clear calculated gradients\n",
    "\n",
    "    x = x.to(device)\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "      output = model(x)\n",
    "      all_loss = model.loss(kld_weight=1.0)\n",
    "      loss = all_loss[\"loss\"]\n",
    "    \n",
    "    # backpropo loss and accumuate loss stat\n",
    "    scaler.scale(loss).backward()    \n",
    "    \n",
    "    training_loss += loss.detach().item() # otherwise this would be a tensor\n",
    "    reconstruction_loss += all_loss['Reconstruction_Loss'].detach().item()\n",
    "    kld_loss += all_loss['KLD'].detach().item()\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "  # let scheduler know it's the next epoch\n",
    "  training_loss /= len(train_dataloader)\n",
    "  reconstruction_loss /= len(train_dataloader)\n",
    "  kld_loss /= len(train_dataloader)\n",
    "    \n",
    "  scheduler.step(training_loss)\n",
    "  \n",
    "  log_str = json.dumps({\n",
    "    \"Epoch\": epoch,\n",
    "    \"training loss\": round(training_loss, 6),\n",
    "    \"reconstruction loss\": round(reconstruction_loss, 6),\n",
    "    \"KLD loss\": round(kld_loss, 6),\n",
    "    \"Learning rate\": scheduler._last_lr\n",
    "  })\n",
    "\n",
    "#   log_str = f\"Epoch {epoch}: training loss {training_loss:.6f}, \" +\\\n",
    "#             f\"reconstruction loss {reconstruction_loss:.6f}, kld_loss {kld_loss:.6f}\"+\\\n",
    "#             f\" Learning Rate: {scheduler._last_lr}\"\n",
    " \n",
    "  with open(f\"{model_store}/{model_id}/training_logs.txt\", \"a\") as log_file:\n",
    "    log_file.write(log_str + \"\\n\")\n",
    "  print(log_str, file=sys.stderr)\n",
    "  \n",
    "  torch.save(StoredModel(model, optimizer, scheduler, None),\n",
    "             f\"{model_store}/{model_id}/epoch_{epoch:02d}\" +\\\n",
    "             f\"_tr-loss_{training_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9rpNw9ZoasF",
    "outputId": "5e8ce878-9137-48e7-a6b4-040f83454fbb"
   },
   "outputs": [],
   "source": [
    "validataion_dataloader_args = dict(batch_size=128,\n",
    "                             num_workers=0 if is_windows else 2) if has_cuda else dict(batch_size=64)\n",
    "validataion_dataloader_args[\"shuffle\"] = False\n",
    "\n",
    "validataion_dataloader = data.DataLoader(dataset, **validataion_dataloader_args)\n",
    "\n",
    "# set model in training mode\n",
    "model.eval()\n",
    "\n",
    "latent_mu = list()\n",
    "latent_log_var = list()\n",
    "\n",
    "for i, x in enumerate(tqdm(validataion_dataloader, desc=\"Validate\")):\n",
    "  x = x.to(device)\n",
    "\n",
    "  _, mus, log_vars = model.encode(x)\n",
    "  latent_mu.append(mus.detach().cpu())\n",
    "  latent_log_var.append(log_vars.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iiXKRd6oasF"
   },
   "outputs": [],
   "source": [
    "torch.save((torch.vstack(latent_mu), torch.vstack(latent_log_var)), f\"latent_vectors/{model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNbtdGmCoasG",
    "outputId": "6f02fe17-8991-4528-c6a1-d7818e0f86a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L2: 100%|██████████| 191880/191880 [48:40<00:00, 65.69it/s]\n"
     ]
    }
   ],
   "source": [
    "L2_divergence_raw = list()\n",
    "\n",
    "image_1 = dataset[0].to(device)\n",
    "\n",
    "for i in tqdm(range(len(dataset) - 1), desc=\"L2\"):\n",
    "  image_2 = dataset[i + 1].to(device)\n",
    "  \n",
    "  diff = (image_1 - image_2).flatten()\n",
    "  \n",
    "  L2_divergence_raw.append(torch.linalg.norm(diff, 2).cpu().item())\n",
    "  \n",
    "  image_1 = image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSqk_96uoasG"
   },
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(L2_divergence_raw), f\"temp_store/{model_id}/l2_divergence_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XouHiYfkoasH",
    "outputId": "ff7d940f-cf72-4a67-d608-2f649476fdb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.88896179199219"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = lambda X, mn, mx: [(x - mn)/(mx - mn) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmd6yOEkoasH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "many2one-jry.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_11785)",
   "language": "python",
   "name": "conda_11785"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
